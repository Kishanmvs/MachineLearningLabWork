# MachineLearningLabWork
**Lab 1**

**Wine Quality Classification (K-Fold Testing)**

*ðŸ“‚ Dataset*

Red Wine Quality dataset (winequality-red.csv) - 1599 samples, 11 features

Binary target: Good (â‰¥7) vs. Not Good (<7)

*âš™ï¸ Method*

Algorithm: Logistic Regression

Validation: Stratified 5-Fold Cross-Validation

*ðŸ“Š Results*

Fold Accuracies: [0.875, 0.881, 0.900, 0.856, 0.893]

Mean Accuracy: 0.88

Std Dev: 0.015

*ðŸ“ Insight*

Model performs well overall.

Imbalance â†’ lower recall for â€œGoodâ€ wine


**Lab 2**

**Spam Email Classification â€” Gaussian Naive Bayes**

*ðŸ“‚ Dataset*

UCI Spambase Dataset

Rows: 4601 emails

Features: 57

Target:

  * `1` â†’ Spam
  * `0` â†’ Not Spam

*âš™ï¸ Method*

Approach: Gaussian Naive Bayes

Split: 70% Training / 30% Testing

Metrics: Accuracy + Confusion Matrix

Implementation: From-scratch Bayes + scikit-learn baseline comparison

*ðŸ“Š Results*

Accuracy: ~0.88

Confusion Matrix:

  ```
  [[ 625  179] 
   [  38 539]]
  ```
Spam Count: 1813

Non-Spam Count: 2788


ðŸ“ Insight

Good baseline performance with simple Gaussian NB

Some false positives â†’ misclassification of legitimate emails as spam

Could improve with feature scaling, text preprocessing, or TF-IDF & Multinomial/Bernoulli NB models




